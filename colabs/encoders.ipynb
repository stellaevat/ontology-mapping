{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellaevat/ontology-mapping/blob/main/colabs/encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg1BOxf1KlAU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQPDnoD3mcxf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFVyasHU3Hk7"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import random\n",
        "import itertools\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import get_scheduler, AutoModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EeqaVxWfb_6"
      },
      "outputs": [],
      "source": [
        "def full_determinism(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read/Write from file"
      ],
      "metadata": {
        "id": "xoSqzMZ_smNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfr-jwzWfy5c"
      },
      "outputs": [],
      "source": [
        "def read_bi_tokenized_from_file(filepath):\n",
        "  s_input_ids, s_token_type_ids, s_attention_mask, t_input_ids, t_token_type_ids, t_attention_mask, labels = [], [], [], [], [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[]\").split(\",\"))) for s in strings[:-1]]\n",
        "      s_input_id, s_token_type_id, s_attention, t_input_id, t_token_type_id, t_attention = lists\n",
        "      label = int(strings[-1])\n",
        "\n",
        "      s_input_ids.append(s_input_id)\n",
        "      s_token_type_ids.append(s_token_type_id)\n",
        "      s_attention_mask.append(s_attention)\n",
        "      t_input_ids.append(t_input_id)\n",
        "      t_token_type_ids.append(t_token_type_id)\n",
        "      t_attention_mask.append(t_attention)\n",
        "      labels.append(label)\n",
        "\n",
        "  source_tokenized = {\"input_ids\" : s_input_ids,\n",
        "                      \"token_type_ids\" : s_token_type_ids,\n",
        "                      \"attention_mask\" : s_attention_mask}\n",
        "  target_tokenized = {\"input_ids\" : t_input_ids,\n",
        "                      \"token_type_ids\" : t_token_type_ids,\n",
        "                      \"attention_mask\" : t_attention_mask}\n",
        "  return source_tokenized, target_tokenized, labels\n",
        "\n",
        "def read_cross_tokenized_from_file(filepath):\n",
        "  input_ids, token_type_ids, attention_mask, labels = [], [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[]\").split(\",\"))) for s in strings[:-1]]\n",
        "      input_id, token_type_id, attention = lists\n",
        "      label = int(strings[-1])\n",
        "\n",
        "      input_ids.append(input_id)\n",
        "      token_type_ids.append(token_type_id)\n",
        "      attention_mask.append(attention)\n",
        "      labels.append(label)\n",
        "\n",
        "  tokenized = {\"input_ids\" : input_ids,\n",
        "               \"token_type_ids\" : token_type_ids,\n",
        "               \"attention_mask\" : attention_mask}\n",
        "  return tokenized, labels\n",
        "\n",
        "def read_onto_tokenized_from_file(filepath):\n",
        "  input_ids, token_type_ids, attention_mask = [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[]\").split(\",\"))) for s in strings]\n",
        "      input_id, token_type_id, attention = lists\n",
        "\n",
        "      input_ids.append(input_id)\n",
        "      token_type_ids.append(token_type_id)\n",
        "      attention_mask.append(attention)\n",
        "\n",
        "  tokenized = {\"input_ids\" : input_ids,\n",
        "               \"token_type_ids\" : token_type_ids,\n",
        "               \"attention_mask\" : attention_mask}\n",
        "  return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_cross_encoder_predictions(predictions, filepath):\n",
        "  with open(filepath, \"w\") as f:\n",
        "    for (i, prediction) in predictions:\n",
        "      f.write(f\"{i},{prediction}\\n\")\n",
        "\n",
        "def write_cross_experiment_metrics(experiment, metrics, filepath):\n",
        "  experiment_str = \",\".join(experiment)\n",
        "  metrics_str = f'{metrics.get(\"test_loss\")},{metrics.get(\"test_accuracy\")},{metrics.get(\"test_precision\")},{metrics.get(\"test_recall\")},{metrics.get(\"test_f1\")}\\n'\n",
        "  entry = \",\".join([experiment_str, metrics_str])\n",
        "  with open(filepath, 'a') as f:\n",
        "    f.write(entry)\n",
        "\n",
        "def write_bi_encoder_predictions(predictions, filepath):\n",
        "  with open(filepath, \"w\") as f:\n",
        "    for (i, prediction) in predictions:\n",
        "      f.write(f\"{i},{prediction}\\n\")\n",
        "\n",
        "def write_experiment_metrics(experiment, metrics, filepath):\n",
        "  experiment_str = \",\".join(experiment)\n",
        "  metrics_str = f'{metrics.get(\"Test loss\")},{metrics.get(\"accuracy\")},{metrics.get(\"precision\")},{metrics.get(\"recall\")},{metrics.get(\"f1\")}\\n'\n",
        "  entry = \",\".join([experiment_str, metrics_str])\n",
        "  with open(filepath, 'a') as f:\n",
        "    f.write(entry)"
      ],
      "metadata": {
        "id": "u2JHBXrPsp3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9cDB8uOQeGe"
      },
      "source": [
        "# Collate input dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00blfsjpIDl5"
      },
      "outputs": [],
      "source": [
        "def filter_source_target(Xi, source, target):\n",
        "  X_source = {k : [v[i] for i in Xi] for (k, v) in source.items()}\n",
        "  X_target = {k : [v[i] for i in Xi] for (k, v) in target.items()}\n",
        "  return X_source, X_target\n",
        "\n",
        "\n",
        "def collate_dataset(X_train, X_val, X_test, y_train, y_val, y_test, Xi_train, Xi_val, Xi_test, label=\"labels\"):\n",
        "  dataset_train = Dataset.from_dict(X_train | {label : y_train, \"indices\" : Xi_train})\n",
        "  dataset_val = Dataset.from_dict(X_val | {label : y_val, \"indices\" : Xi_val})\n",
        "  dataset_test = Dataset.from_dict(X_test | {label : y_test, \"indices\" : Xi_test})\n",
        "  dataset = DatasetDict({\n",
        "      'train' : dataset_train,\n",
        "      'val' : dataset_val,\n",
        "      'test' : dataset_test\n",
        "  })\n",
        "  dataset.set_format(type=\"torch\")\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def collate_ontology_dataset(onto_tokenized, purpose=\"test\"):\n",
        "  size = len(onto_tokenized[\"input_ids\"])\n",
        "  print(f\"Ontology size: {size}\")\n",
        "\n",
        "  dataset_onto = Dataset.from_dict(onto_tokenized | {\"indices\" : list(range(size))})\n",
        "  dataset = DatasetDict({purpose : dataset_onto})\n",
        "  dataset.set_format(type=\"torch\")\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_bi_datasets_from_tokenized(source_tokenized, target_tokenized, labels):\n",
        "  Xi = np.arange(len(labels))\n",
        "  y = np.array(labels)\n",
        "  Xi_train_val, Xi_test, y_train_val, y_test = train_test_split(Xi, y, test_size=0.2, random_state=3)\n",
        "  Xi_train, Xi_val, y_train, y_val = train_test_split(Xi_train_val, y_train_val, test_size=0.25, random_state=3)\n",
        "\n",
        "  X_source_train, X_target_train = filter_source_target(Xi_train, source_tokenized, target_tokenized)\n",
        "  X_source_val, X_target_val = filter_source_target(Xi_val, source_tokenized, target_tokenized)\n",
        "  X_source_test, X_target_test = filter_source_target(Xi_test, source_tokenized, target_tokenized)\n",
        "\n",
        "  print(f\"Train: {len(y_train)}\")\n",
        "  print(f\"Validation: {len(y_val)}\")\n",
        "  print(f\"Test: {len(y_test)}\")\n",
        "\n",
        "  source_data = collate_dataset(X_source_train, X_source_val, X_source_test, y_train, y_val, y_test, Xi_train, Xi_val, Xi_test)\n",
        "  target_data = collate_dataset(X_target_train, X_target_val, X_target_test, y_train, y_val, y_test, Xi_train, Xi_val, Xi_test)\n",
        "  return source_data, target_data\n",
        "\n",
        "def get_cross_dataset_from_tokenized(tokenized, labels):\n",
        "  Xi = np.arange(len(labels))\n",
        "  y = np.array(labels)\n",
        "  Xi_train_val, Xi_test, y_train_val, y_test = train_test_split(Xi, y, test_size=0.2, random_state=3)\n",
        "  Xi_train, Xi_val, y_train, y_val = train_test_split(Xi_train_val, y_train_val, test_size=0.25, random_state=3)\n",
        "\n",
        "  X_train = {k : [v[i] for i in Xi_train] for (k, v) in tokenized.items()}\n",
        "  X_val = {k : [v[i] for i in Xi_val] for (k, v) in tokenized.items()}\n",
        "  X_test = {k : [v[i] for i in Xi_test] for (k, v) in tokenized.items()}\n",
        "\n",
        "  print(f\"Train: {len(y_train)}\")\n",
        "  print(f\"Validation: {len(y_val)}\")\n",
        "  print(f\"Test: {len(y_test)}\")\n",
        "\n",
        "  dataset = collate_dataset(X_train, X_val, X_test, y_train, y_val, y_test, Xi_train, Xi_val, Xi_test, label=\"label\")\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwy-uRdLOY69"
      },
      "source": [
        "# Bi-encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "lEoZj6zC0UyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKdO0rN_wJsY"
      },
      "outputs": [],
      "source": [
        "class BiEncoderForSequenceClassification(torch.nn.Module):\n",
        "  def __init__(self, model_name, num_labels, id2label=None, label2id=None, token_embeddings_size=None, hidden_layer=-1):\n",
        "    super().__init__()\n",
        "    self.source_model = AutoModel.from_pretrained(model_name)\n",
        "    self.target_model = AutoModel.from_pretrained(model_name)\n",
        "    if token_embeddings_size:\n",
        "      self.source_model.resize_token_embeddings(token_embeddings_size)\n",
        "      self.target_model.resize_token_embeddings(token_embeddings_size)\n",
        "\n",
        "    self.num_labels = num_labels\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "    self.loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      s_input_ids=None, t_input_ids=None,\n",
        "      s_attention_mask=None, t_attention_mask=None,\n",
        "      s_token_type_ids=None, t_token_type_ids=None,\n",
        "      s_position_ids=None, t_position_ids=None,\n",
        "      s_head_mask=None, t_head_mask=None,\n",
        "      s_inputs_embeds=None, t_inputs_embeds=None,\n",
        "      labels=None\n",
        "    ):\n",
        "\n",
        "    source_outputs = self.source_model(\n",
        "      s_input_ids,\n",
        "      attention_mask=s_attention_mask,\n",
        "      token_type_ids=s_token_type_ids,\n",
        "      position_ids=s_position_ids,\n",
        "      head_mask=s_head_mask,\n",
        "      inputs_embeds=s_inputs_embeds,\n",
        "    )\n",
        "\n",
        "    target_outputs = self.target_model(\n",
        "      t_input_ids,\n",
        "      attention_mask=t_attention_mask,\n",
        "      token_type_ids=t_token_type_ids,\n",
        "      position_ids=t_position_ids,\n",
        "      head_mask=t_head_mask,\n",
        "      inputs_embeds=t_inputs_embeds,\n",
        "    )\n",
        "\n",
        "    pooled_source_outputs = self.dropout(source_outputs[1])\n",
        "    pooled_target_outputs = self.dropout(target_outputs[1])\n",
        "\n",
        "    # Dot product of source and corresponding target embeddings\n",
        "    logits = torch.sum(pooled_source_outputs * pooled_target_outputs, dim=-1)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss = self.loss(logits.view(-1), labels.view(-1).float())\n",
        "\n",
        "    return SequenceClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "  def get_source_model_outputs(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    with torch.no_grad():\n",
        "      source_outputs = self.source_model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "    return source_outputs[1]\n",
        "\n",
        "  def get_target_model_outputs(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    with torch.no_grad():\n",
        "      target_outputs = self.target_model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "    return target_outputs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWaF541oJfVv"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "  def __init__(self, learning_rate, batch_size, patience=1, delta=0):\n",
        "    self.patience = patience\n",
        "    self.delta = delta\n",
        "    self.counter = 0\n",
        "    self.min_loss = float('inf')\n",
        "\n",
        "    self.best_epoch = 0\n",
        "    self.best_metrics = {}\n",
        "    self.best_model_state = None\n",
        "    self.best_optimizer_state = None\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def early_stop(self, loss, epoch, model_state, optimizer_state, metrics):\n",
        "    if loss < self.min_loss:\n",
        "      self.min_loss = loss\n",
        "      self.counter = 0\n",
        "\n",
        "      self.best_epoch = epoch\n",
        "      self.best_metrics = metrics\n",
        "      self.best_model_state = model_state\n",
        "      self.best_optimizer_state = optimizer_state\n",
        "\n",
        "    elif loss >= (self.min_loss + self.delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def save_best_performance(self, filepath):\n",
        "    performance = f\"{self.batch_size},{self.learning_rate},{self.best_epoch},{self.min_loss},{self.best_metrics.get('accuracy')},{self.best_metrics.get('precision')},{self.best_metrics.get('recall')},{self.best_metrics.get('f1')}\\n\"\n",
        "    print(performance)\n",
        "    with open(filepath, 'a') as f:\n",
        "      f.write(performance)\n",
        "\n",
        "  def save_best_checkpoint(self, filepath):\n",
        "    torch.save({\n",
        "      'model_state_dict' : self.best_model_state,\n",
        "      'optimizer_state_dict' : self.best_optimizer_state,\n",
        "      'batch_size' : self.batch_size,\n",
        "      'learning_rate': self.learning_rate,\n",
        "      'epoch' : self.best_epoch,\n",
        "      'loss' : self.min_loss,\n",
        "      'accuracy' : self.best_metrics.get('accuracy'),\n",
        "      'precision' : self.best_metrics.get('precision'),\n",
        "      'recall' : self.best_metrics.get('recall'),\n",
        "      'f1' : self.best_metrics.get('f1'),\n",
        "      }, filepath)\n",
        "\n",
        "  def restore_model_state(self):\n",
        "    return self.best_model_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QFIQmGhGda9"
      },
      "outputs": [],
      "source": [
        "def show_results(epoch, loss, metrics):\n",
        "  print(f\"\\n\\nEPOCH {epoch}\\n\")\n",
        "  print(f\"Training loss: {loss}\")\n",
        "  pprint(metrics)\n",
        "  print()\n",
        "\n",
        "def evaluate_biencoder(model, source_data, target_data, batch_size=32, testing=False, verbose=False):\n",
        "  split = \"test\" if testing else \"val\"\n",
        "  eval_dataloader_index = DataLoader(Dataset.from_dict({'index' : range(len(source_data[split]))}), batch_size=batch_size)\n",
        "  metrics = [evaluate.load('accuracy'), evaluate.load('precision'), evaluate.load('recall'), evaluate.load('f1')]\n",
        "  all_indices, all_predictions = [], []\n",
        "\n",
        "  model.eval()\n",
        "  avg_loss = 0\n",
        "  for batch in eval_dataloader_index:\n",
        "    batch_index = list(batch[\"index\"])\n",
        "    source_batch = source_data[split][batch_index]\n",
        "    target_batch = target_data[split][batch_index]\n",
        "    labels = source_batch[\"labels\"]\n",
        "    indices = source_batch[\"indices\"]\n",
        "\n",
        "    source_batch = {\"s_\" + k: v.to(device) for (k, v) in source_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "    target_batch = {\"t_\" + k: v.to(device) for (k, v) in target_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "    params = source_batch | target_batch\n",
        "    params[\"labels\"] = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**params)\n",
        "\n",
        "    logits = outputs.logits.cpu()\n",
        "    loss = outputs.loss.item()\n",
        "    avg_loss += loss * len(batch_index)\n",
        "\n",
        "    predictions = np.where(logits.squeeze() >= 0.5, 1, 0)\n",
        "    for metric in metrics:\n",
        "      metric.add_batch(predictions=predictions, references=labels.cpu())\n",
        "\n",
        "    all_indices.extend(indices.tolist())\n",
        "    all_predictions.extend(predictions.tolist())\n",
        "\n",
        "  avg_loss = avg_loss / len(source_data[split])\n",
        "  metric_dict = {(\"Test loss\" if testing else \"Validation loss\") : avg_loss}\n",
        "  metric_dict.update(metrics[0].compute())\n",
        "  for metric in metrics[1:]:\n",
        "    metric_dict.update(metric.compute(average='macro'))\n",
        "\n",
        "  if verbose:\n",
        "    pprint(metric_dict)\n",
        "\n",
        "  if testing:\n",
        "    return metric_dict, list(zip(all_indices, all_predictions))\n",
        "\n",
        "  return metric_dict\n",
        "\n",
        "\n",
        "def train_biencoder(model, source_data, target_data,\n",
        "                    epochs=10, batch_size=32, learning_rate=1e-5,\n",
        "                    save_performance=False, save_checkpoint=False,\n",
        "                    early_stopping=True,\n",
        "                    verbose=True):\n",
        "  train_dataloader_index = DataLoader(Dataset.from_dict({'index' : range(len(source_data[\"train\"]))}), shuffle=True, batch_size=batch_size)\n",
        "  num_training_steps = epochs * len(train_dataloader_index)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "  scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "  early_stopper = EarlyStopper(patience=3, learning_rate=learning_rate, batch_size=batch_size)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps), position=0, leave=True)\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(1, epochs+1):\n",
        "    avg_loss = 0\n",
        "    for batch in train_dataloader_index:\n",
        "      batch_index = list(batch[\"index\"])\n",
        "      source_batch = source_data[\"train\"][batch_index]\n",
        "      target_batch = target_data[\"train\"][batch_index]\n",
        "      labels = source_batch[\"labels\"]\n",
        "\n",
        "      source_batch = {\"s_\" + k: v.to(device) for (k, v) in source_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "      target_batch = {\"t_\" + k: v.to(device) for (k, v) in target_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "      params = source_batch | target_batch\n",
        "      params[\"labels\"] = labels.to(device)\n",
        "\n",
        "      outputs = model(**params)\n",
        "      loss = outputs.loss\n",
        "      avg_loss += loss * len(batch[\"index\"])\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    avg_loss = avg_loss / len(source_data[\"train\"])\n",
        "    metrics = evaluate_biencoder(model, source_data, target_data, batch_size=batch_size)\n",
        "    if verbose:\n",
        "      show_results(epoch, avg_loss, metrics)\n",
        "\n",
        "    val_loss = metrics[\"Validation loss\"]\n",
        "    if early_stopper.early_stop(val_loss, epoch, model.state_dict(), optimizer.state_dict(), metrics) and early_stopping:\n",
        "      break\n",
        "    if epoch == 3:\n",
        "      break\n",
        "\n",
        "  model.load_state_dict(early_stopper.restore_model_state())\n",
        "\n",
        "  if save_performance:\n",
        "    early_stopper.save_best_performance(save_performance)\n",
        "  if save_checkpoint:\n",
        "    early_stopper.save_best_checkpoint(save_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbVIcapntHE"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IebuX4BGz0J"
      },
      "outputs": [],
      "source": [
        "features = ['term', 'int', 'ext']\n",
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "direction = \"ncit2doid\"\n",
        "\n",
        "feature = features[1]\n",
        "negatives = negative_sampling[2]\n",
        "\n",
        "source_tokenized, target_tokenized, labels = read_bi_tokenized_from_file(f\"bi_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "source_data, target_data = get_bi_datasets_from_tokenized(source_tokenized, target_tokenized, labels)\n",
        "\n",
        "param_grid = ParameterGrid({\n",
        "    \"batch_size\" : [2 ** i for i in range(4, 7)],\n",
        "    \"learning_rate\" : [10 ** i for i in range(-6, -2)]\n",
        "})\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "for params in list(param_grid)[::-1]:\n",
        "  print(params)\n",
        "\n",
        "  full_determinism(seed=3)\n",
        "  model = BiEncoderForSequenceClassification(pretrained, num_labels=1)\n",
        "  model.to(device)\n",
        "\n",
        "  save_performance = f\"bi_performance_{feature}_{negatives}.csv\"\n",
        "  train_biencoder(\n",
        "      model, source_data, target_data,\n",
        "      save_performance=save_performance,\n",
        "      verbose=True,\n",
        "      **params\n",
        "  )\n",
        "\n",
        "  model.cpu()\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "59mHI6mrc1y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_params = {\n",
        "    (\"term\", \"random\")    : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"term\", \"multi\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"term\", \"neighbour\") : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"int\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-04},  # 3 epochs\n",
        "    (\"int\", \"multi\")      : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"int\", \"neighbour\")  : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"ext\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 5 epochs\n",
        "    (\"ext\", \"multi\")      : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 7 epochs\n",
        "    (\"ext\", \"neighbour\")  : {\"batch_size\" : 64, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "}\n",
        "\n",
        "features = ['term', 'int', 'ext']\n",
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "direction = \"ncit2doid\"\n",
        "directory = \"\"\n",
        "\n",
        "feature = features[1]\n",
        "negatives = negative_sampling[2]\n",
        "load_from_file = True\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "full_determinism(seed=3)\n",
        "model = BiEncoderForSequenceClassification(pretrained, num_labels=1)\n",
        "if load_from_file:\n",
        "  checkpoint = torch.load(f\"{directory}/checkpoints/bi_checkpoint_{feature}_{negatives}.pt\")\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "experiment = (feature, negatives)\n",
        "params = tuned_params[experiment]\n",
        "\n",
        "source_tokenized, target_tokenized, labels = read_bi_tokenized_from_file(f\"{directory}/tokenized/bi_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "source_data, target_data = get_bi_datasets_from_tokenized(source_tokenized, target_tokenized, labels)\n",
        "\n",
        "if not load_from_file:\n",
        "  save_checkpoint = f\"{directory}/checkpoints/bi_checkpoint_{feature}_{negatives}.pt\"\n",
        "  train_biencoder(\n",
        "      model, source_data, target_data,\n",
        "      save_checkpoint=save_checkpoint,\n",
        "      verbose=False,\n",
        "      **params\n",
        "  )\n",
        "  torch.save({\"model_state_dict\" : model.state_dict()}, save_checkpoint)"
      ],
      "metadata": {
        "id": "5IgidZ2GdCn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set"
      ],
      "metadata": {
        "id": "L3m5c6yGJ0Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_determinism(seed=3)\n",
        "\n",
        "metrics, predictions = evaluate_biencoder(\n",
        "    model, source_data, target_data,\n",
        "    batch_size=params[\"batch_size\"],\n",
        "    testing=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "write_bi_encoder_predictions(predictions, f\"{directory}/predictions/bi_predictions_{feature}_{negatives}.csv\")\n",
        "write_experiment_metrics(experiment, metrics, f\"{directory}/predictions/bi_test_metrics.csv\")"
      ],
      "metadata": {
        "id": "jGzLHgqRNTb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get embeddings"
      ],
      "metadata": {
        "id": "RQpr5oFLJMPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_bi_encoder_embeddings(embeddings, filepath):\n",
        "  with open(filepath, \"a\") as f:\n",
        "    for (i, source, target) in embeddings:\n",
        "      if source is None:\n",
        "        f.write(f\"{i},{target.tolist()}\\n\")\n",
        "      else:\n",
        "        f.write(f\"{i},{source.tolist()},{target.tolist()}\\n\")\n",
        "\n",
        "def generate_biencoder_embeddings(model, target_data, source_data=None, split=\"test\", batch_size=32, filepath=\"bi_embeddings.csv\"):\n",
        "  if source_data is not None:\n",
        "    target_positive = target_data[split].filter(lambda row: row[\"labels\"] == 1)\n",
        "    source_positive = source_data[split].filter(lambda row: row[\"labels\"] == 1)\n",
        "  else:\n",
        "    target_positive = target_data[split]\n",
        "\n",
        "  dataloader_index = DataLoader(Dataset.from_dict({'index' : range(len(target_positive))}), shuffle=False, batch_size=batch_size)\n",
        "  progress_bar = tqdm(range(len(dataloader_index)), position=0, leave=True)\n",
        "\n",
        "  model.eval()\n",
        "  for batch in dataloader_index:\n",
        "    batch_index = list(batch[\"index\"])\n",
        "    target_batch = target_positive[batch_index]\n",
        "    if source_data is not None:\n",
        "      source_batch = source_positive[batch_index]\n",
        "    indices = target_positive[batch_index][\"indices\"]\n",
        "\n",
        "    target_batch = {k: v.to(device) for (k, v) in target_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "    if source_data is not None:\n",
        "      source_batch = {k: v.to(device) for (k, v) in source_batch.items() if k not in [\"labels\", \"indices\"]}\n",
        "\n",
        "    target_embeddings = model.get_target_model_outputs(**target_batch)\n",
        "    if source_data is not None:\n",
        "      source_embeddings = model.get_source_model_outputs(**source_batch)\n",
        "    else:\n",
        "      source_embeddings = [None] * len(target_embeddings)\n",
        "\n",
        "    write_bi_encoder_embeddings(zip(indices, source_embeddings, target_embeddings), filepath)\n",
        "\n",
        "    target_embeddings.cpu()\n",
        "    if source_data is not None:\n",
        "      source_embeddings.cpu()\n",
        "    del target_embeddings\n",
        "    del source_embeddings\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    progress_bar.update(1)"
      ],
      "metadata": {
        "id": "bHDrhv99JOPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_params = {\n",
        "    (\"term\", \"random\")    : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"term\", \"multi\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"term\", \"neighbour\") : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"int\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-04},  # 3 epochs\n",
        "    (\"int\", \"multi\")      : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"int\", \"neighbour\")  : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"ext\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 5 epochs\n",
        "    (\"ext\", \"multi\")      : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 7 epochs\n",
        "    (\"ext\", \"neighbour\")  : {\"batch_size\" : 64, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "}\n",
        "\n",
        "features = ['term', 'int', 'ext']\n",
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "direction = \"ncit2doid\"\n",
        "load_from_file = True\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "for (feature, negatives) in list(itertools.product(features, negative_sampling))[4:]:\n",
        "  print(feature, negatives)\n",
        "\n",
        "  full_determinism(seed=3)\n",
        "  model = BiEncoderForSequenceClassification(pretrained, num_labels=1)\n",
        "  if load_from_file:\n",
        "    checkpoint = torch.load(f\"{directory}/checkpoints/bi-checkpoints/bi_checkpoint_{feature}_{negatives}.csv\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  model.to(device)\n",
        "\n",
        "  experiment = (feature, negatives)\n",
        "  params = tuned_params[experiment]\n",
        "  split = \"test\"\n",
        "\n",
        "\n",
        "  # Query embeddings\n",
        "  source_tokenized, target_tokenized, labels = read_bi_tokenized_from_file(f\"{directory}/tokenized/bi_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "  source_data, target_data = get_bi_datasets_from_tokenized(source_tokenized, target_tokenized, labels)\n",
        "  target_positive = target_data[split].filter(lambda row: row[\"labels\"] == 1)\n",
        "\n",
        "  bi_embeddings_file = f\"{directory}/embeddings/bi_{split}_embeddings_{feature}_{negatives}.csv\"\n",
        "  generate_biencoder_embeddings(model, target_data, source_data, split=split, batch_size=params[\"batch_size\"], filepath=bi_embeddings_file)\n",
        "\n",
        "\n",
        "  # Hard embeddings\n",
        "  if negatives != \"neighbour\":\n",
        "    hard_source_tokenized, hard_target_tokenized, hard_labels = read_bi_tokenized_from_file(f\"{directory}/tokenized/bi_tokenized_{feature}_neighbour_{direction}.csv\")\n",
        "    hard_source_data, hard_target_data = get_bi_datasets_from_tokenized(hard_source_tokenized, hard_target_tokenized, hard_labels)\n",
        "  else:\n",
        "    hard_source_data, hard_target_data = source_data, target_data\n",
        "\n",
        "  to_drop = []\n",
        "  for i, index in enumerate(hard_target_data[split][\"indices\"]):\n",
        "    if index in target_positive[\"indices\"]:\n",
        "      to_drop.append(i)\n",
        "  hard_target_data_clean = DatasetDict({split : hard_target_data[split].select([i for i in range(len(hard_target_data[split])) if i not in to_drop])})\n",
        "\n",
        "  hard_embeddings_file = f\"{directory}/embeddings/hard_embeddings_{feature}_{negatives}.csv\"\n",
        "  generate_biencoder_embeddings(model, target_data=hard_target_data_clean, split=\"test\", batch_size=params[\"batch_size\"]*2, filepath=hard_embeddings_file)\n",
        "\n",
        "\n",
        "  # Random embeddings\n",
        "  if negatives != \"multi\":\n",
        "    random_source_tokenized, random_target_tokenized, random_labels = read_bi_tokenized_from_file(f\"{directory}/tokenized/bi_tokenized_{feature}_multi_{direction}.csv\")\n",
        "    random_source_data, random_target_data = get_bi_datasets_from_tokenized(random_source_tokenized, random_target_tokenized, random_labels)\n",
        "  else:\n",
        "    random_source_data, random_target_data = source_data, target_data\n",
        "\n",
        "  to_drop = []\n",
        "  for i, index in enumerate(random_target_data[split][\"indices\"]):\n",
        "    if index in target_positive[\"indices\"]:\n",
        "      to_drop.append(i)\n",
        "  random_target_data_clean = DatasetDict({split : random_target_data[split].select([i for i in range(len(random_target_data[split])) if i not in to_drop])})\n",
        "\n",
        "  random_embeddings_file = f\"{directory}/embeddings/random_embeddings_{feature}_{negatives}.csv\"\n",
        "  generate_biencoder_embeddings(model, target_data=random_target_data_clean, split=\"test\", batch_size=params[\"batch_size\"]*2, filepath=random_embeddings_file)\n",
        "\n",
        "\n",
        "  # # Ontology embeddings\n",
        "  onto_tokenized = read_onto_tokenized_from_file(f\"{directory}/tokenized/doid_tokenized_{feature}_{negatives}.csv\")\n",
        "  onto_data = collate_ontology_dataset(onto_tokenized, purpose=split)\n",
        "\n",
        "\n",
        "  to_drop = []\n",
        "  for i, tokenized in tqdm(enumerate(onto_data[split][\"input_ids\"])):\n",
        "    for target in target_positive[\"input_ids\"]:\n",
        "      if np.array_equal(tokenized, target):\n",
        "        to_drop.append(i)\n",
        "        break\n",
        "  onto_data[split] = onto_data[split].select([i for i in range(len(onto_data[split])) if i not in to_drop])\n",
        "\n",
        "  onto_embeddings_file = f\"{directory}/embeddings/doid_embeddings_{feature}_{negatives}.csv\"\n",
        "  generate_biencoder_embeddings(model, target_data=onto_data, split=split, batch_size=params[\"batch_size\"]*2, filepath=onto_embeddings_file)\n",
        "\n",
        "  print()\n",
        "\n",
        "  model.cpu()\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "eyG3VgTRgh_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-encoder"
      ],
      "metadata": {
        "id": "zSiW70HASrOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "JXVJNBXW0YFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cross_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  metrics = [evaluate.load('accuracy'), evaluate.load('precision'), evaluate.load('recall'), evaluate.load('f1')]\n",
        "  metric_dict = metrics[0].compute(predictions=predictions, references=labels)\n",
        "  for metric in metrics[1:]:\n",
        "    metric_dict.update(metric.compute(predictions=predictions, references=labels, average='macro'))\n",
        "\n",
        "  return metric_dict\n",
        "\n",
        "def prepare_model(model, tokenized_data, learning_rate=1e-5, batch_size=32, epochs=10):\n",
        "  pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"testing\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_only_model=True,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    logging_steps=1,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    load_best_model_at_end=True,\n",
        "    seed=3,\n",
        "    data_seed=3,\n",
        "    full_determinism=True,\n",
        "    no_cuda=False\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data['train'],\n",
        "    eval_dataset=tokenized_data['val'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_cross_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "efxOjMVfqRJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ],
      "metadata": {
        "id": "ejK7tRwOxfrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = ParameterGrid({\n",
        "    \"batch_size\" : [2 ** i for i in range(4, 7)],\n",
        "    \"learning_rate\" : [10 ** i for i in range(-6, -2)]\n",
        "})\n",
        "\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "feature, negatives = \"term\", \"random\"\n",
        "direction = \"ncit2doid\"\n",
        "experiment = (feature, negatives)\n",
        "\n",
        "tokenized, labels = read_cross_tokenized_from_file(f\"{directory}/tokenized/cross_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "dataset = get_cross_dataset_from_tokenized(tokenized, labels)\n",
        "indices = dataset[\"test\"][\"indices\"]\n",
        "tokenized_data = {k : v for (k,v) in dataset.items() if k != \"indices\"}\n",
        "\n",
        "for params in list(param_grid)[5:6]:\n",
        "  print(params)\n",
        "  full_determinism(seed=3)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(pretrained, num_labels=2)\n",
        "  trainer = prepare_model(model, tokenized_data, **params)\n",
        "  trainer.train()"
      ],
      "metadata": {
        "id": "aDbz-FsY-b4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set"
      ],
      "metadata": {
        "id": "drCqFZ7Hxmq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_params = {\n",
        "    (\"term\", \"random\")    : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "    (\"term\", \"multi\")     : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "    (\"term\", \"neighbour\") : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 6 epochs\n",
        "    (\"int\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "    (\"int\", \"multi\")      : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 1 epochs\n",
        "    (\"int\", \"neighbour\")  : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "    (\"ext\", \"random\")     : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 1 epochs\n",
        "    (\"ext\", \"multi\")      : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 1 epochs\n",
        "    (\"ext\", \"neighbour\")  : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "}\n",
        "\n",
        "features = ['int']\n",
        "negative_sampling = ['random']\n",
        "direction = \"ncit2doid\"\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "for (feature, negatives) in list(itertools.product(features, negative_sampling)):\n",
        "  print(feature, negatives)\n",
        "\n",
        "  experiment = (feature, negatives)\n",
        "  params = tuned_params[experiment]\n",
        "\n",
        "  tokenized, labels = read_cross_tokenized_from_file(f\"{directory}/tokenized/cross_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "  dataset = get_cross_dataset_from_tokenized(tokenized, labels)\n",
        "  indices = dataset[\"test\"][\"indices\"]\n",
        "  tokenized_data = {k : v for (k,v) in dataset.items() if k != \"indices\"}\n",
        "\n",
        "  full_determinism(seed=3)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(pretrained, num_labels=2)\n",
        "  model.to(device)\n",
        "\n",
        "  trainer = prepare_model(model, tokenized_data, **params)\n",
        "  trainer.train()\n",
        "  trainer.save_model(f\"{directory}/checkpoints/cross_checkpoint_{feature}_{negatives}.pt\")\n",
        "\n",
        "  full_determinism(seed=3)\n",
        "  predictions, label_ids, metrics = trainer.predict(tokenized_data[\"test\"])\n",
        "  print(metrics)\n",
        "  predictions = np.argmax(predictions, axis=1).tolist()\n",
        "\n",
        "  write_cross_encoder_predictions(list(zip(indices, predictions)), f\"{directory}/predictions/cross_predictions_{feature}_{negatives}.csv\")\n",
        "  write_cross_experiment_metrics(experiment, metrics, f\"{directory}/predictions/cross_test_metrics.csv\")\n",
        "\n",
        "  model.cpu()\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "4mlkSybKprfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned search space testing"
      ],
      "metadata": {
        "id": "Qsuoh0qdaIaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_params = {\n",
        "    (\"term\", \"random\")    : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "    (\"term\", \"multi\")     : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"term\", \"neighbour\") : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 4 epochs\n",
        "    (\"int\", \"random\")     : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"int\", \"multi\")      : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"int\", \"neighbour\")  : {\"batch_size\" : 16, \"learning_rate\" : 1e-05},  # 3 epochs\n",
        "    (\"ext\", \"random\")     : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 5 epochs\n",
        "    (\"ext\", \"multi\")      : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 7 epochs\n",
        "    (\"ext\", \"neighbour\")  : {\"batch_size\" : 32, \"learning_rate\" : 1e-05},  # 2 epochs\n",
        "}\n",
        "\n",
        "feature, negatives, direction = 'int', 'random', \"ncit2doid\"\n",
        "experiment = (feature, negatives)\n",
        "params = tuned_params[experiment]\n",
        "cutoff = 25\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "\n",
        "# Train model\n",
        "tokenized, labels = read_cross_tokenized_from_file(f\"{directory}/tokenized/cross_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "dataset = get_cross_dataset_from_tokenized(tokenized, labels)\n",
        "indices = dataset[\"test\"][\"indices\"]\n",
        "tokenized_data = {k : v for (k,v) in dataset.items() if k != \"indices\"}\n",
        "\n",
        "full_determinism(seed=3)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained, num_labels=2)\n",
        "model.to(device)\n",
        "trainer = prepare_model(model, tokenized_data, **params)\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Test model\n",
        "faiss_tokenized, faiss_labels = read_cross_tokenized_from_file(f\"faiss_{cutoff}_tokenized_{feature}_{direction}.csv\")\n",
        "faiss_indices = np.arange(len(labels))\n",
        "faiss_dataset = DatasetDict({\"test\" : Dataset.from_dict(faiss_tokenized | {\"label\" : faiss_labels, \"indices\" : faiss_indices})})\n",
        "faiss_dataset.set_format(type=\"torch\")\n",
        "faiss_prediction_dataset = {k : v for (k,v) in faiss_dataset.items() if k != \"indices\"}\n",
        "\n",
        "full_determinism(seed=3)\n",
        "predictions, label_ids, metrics = trainer.predict(faiss_prediction_dataset[\"test\"])\n",
        "print(metrics)\n",
        "predictions = np.argmax(predictions, axis=1).tolist()\n",
        "\n",
        "write_cross_encoder_predictions(list(zip(indices, predictions)), f\"{directory}/predictions/faiss_{cutoff}_predictions_{feature}_{negatives}.csv\")\n",
        "write_cross_experiment_metrics(experiment, metrics, f\"{directory}/predictions/faiss_{cutoff}_test_metrics.csv\")"
      ],
      "metadata": {
        "id": "sKzFFVcTaPJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "xoSqzMZ_smNU",
        "lEoZj6zC0UyH",
        "JXVJNBXW0YFD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}