{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxwHQYANf6xMODG3DgxczA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellaevat/ontology-mapping/blob/main/colabs/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "t7yN5446Zsbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "1RkkUTt1ZlTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read/Write to file"
      ],
      "metadata": {
        "id": "S0750gKK2eK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cross_sentences(filepath):\n",
        "  sentences, labels = [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      sentence, label = [field.strip('\"') for field in line.strip().split('\",\"')]\n",
        "      sentences.append(sentence)\n",
        "      labels.append(label)\n",
        "  return sentences, labels\n",
        "\n",
        "def read_bi_sentences(filepath):\n",
        "  source_sentences, target_sentences, labels = [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      source_sentence, target_sentence, label = [field.strip('\"') for field in line.strip().split('\",\"')]\n",
        "      source_sentences.append(source_sentence)\n",
        "      target_sentences.append(target_sentence)\n",
        "      labels.append(label)\n",
        "  return source_sentences, target_sentences, labels\n",
        "\n",
        "def read_onto_sentences(filepath):\n",
        "  sentences = []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      sentences.append(line.strip().strip('\"'))\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "a6lbG53yFZfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_bi_encoder_tokenized(tokenized_source, tokenized_target, labels, filepath):\n",
        "  s_input_ids = tokenized_source[\"input_ids\"]\n",
        "  s_token_type_ids = tokenized_source[\"token_type_ids\"]\n",
        "  s_attention_mask = tokenized_source[\"attention_mask\"]\n",
        "  t_input_ids = tokenized_target[\"input_ids\"]\n",
        "  t_token_type_ids = tokenized_target[\"token_type_ids\"]\n",
        "  t_attention_mask = tokenized_target[\"attention_mask\"]\n",
        "\n",
        "  with open(filepath, \"w\") as f:\n",
        "    for (s_input_id, s_token_type_id, s_attention, t_input_id, t_token_type_id, t_attention, label) in zip(\n",
        "        s_input_ids,\n",
        "        s_token_type_ids,\n",
        "        s_attention_mask,\n",
        "        t_input_ids,\n",
        "        t_token_type_ids,\n",
        "        t_attention_mask,\n",
        "        labels\n",
        "      ):\n",
        "      f.write(f\"{s_input_id},{s_token_type_id},{s_attention},{t_input_id},{t_token_type_id},{t_attention},{label}\\n\")\n",
        "\n",
        "def write_cross_encoder_tokenized(tokenized, labels, filepath):\n",
        "  input_ids = tokenized[\"input_ids\"]\n",
        "  token_type_ids = tokenized[\"token_type_ids\"]\n",
        "  attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "  with open(filepath, \"w\") as f:\n",
        "    for (input_id, token_type_id, attention, label) in zip(input_ids, token_type_ids, attention_mask, labels):\n",
        "      f.write(f\"{input_id},{token_type_id},{attention},{label}\\n\")\n",
        "\n",
        "def write_ontology_tokenized(tokenized, filepath):\n",
        "  input_ids = tokenized[\"input_ids\"]\n",
        "  token_type_ids = tokenized[\"token_type_ids\"]\n",
        "  attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "  with open(filepath, \"w\") as f:\n",
        "    for (input_id, token_type_id, attention) in zip(input_ids, token_type_ids, attention_mask):\n",
        "      f.write(f\"{input_id},{token_type_id},{attention}\\n\")"
      ],
      "metadata": {
        "id": "lpwEG0Wr8BXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "KyvV3C4fEGtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_determinism(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "DjP1MD6BHsp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences_for_experimental_setting(feature, negatives, direction=\"ncit2doid\"):\n",
        "  gc.collect()\n",
        "  full_determinism(seed=3)\n",
        "\n",
        "  sources, targets, labels = read_bi_sentences(f\"bi_sentences_{feature}_{negatives}_{direction}.csv\")\n",
        "  combined, labels = read_cross_sentences(f\"cross_sentences_{feature}_{negatives}_{direction}.csv\")\n",
        "  onto = read_onto_sentences(f\"{direction.split('2')[1]}_sentences_{feature}.csv\")\n",
        "  n = len(sources)\n",
        "\n",
        "  tokenized = tokenizer(sources + targets + combined + onto, padding=\"longest\", truncation=True, max_length=512)\n",
        "  tokenized_source = {k : v[:n] for (k, v) in tokenized.items()}\n",
        "  tokenized_target = {k : v[n:2*n] for (k, v) in tokenized.items()}\n",
        "  tokenized_cross = {k : v[2*n:3*n] for (k, v) in tokenized.items()}\n",
        "  tokenized_onto = {k : v[3*n:] for (k, v) in tokenized.items()}\n",
        "\n",
        "  write_bi_encoder_tokenized(tokenized_source, tokenized_target, labels, f\"bi_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "  write_cross_encoder_tokenized(tokenized_cross, labels, f\"cross_tokenized_{feature}_{negatives}_{direction}.csv\")\n",
        "  write_ontology_tokenized(tokenized_onto, f\"{direction.split('2')[1]}_tokenized_{feature}_{negatives}.csv\")\n",
        "\n",
        "  del tokenized_source\n",
        "  del tokenized_target\n",
        "  del tokenized_cross\n",
        "  del tokenized_onto\n",
        "  del tokenized\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "KXS619P9EOxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "features = ['term', 'int', 'ext']\n",
        "direction = \"ncit2doid\"\n",
        "\n",
        "for feature in features:\n",
        "  for negatives in negative_sampling:\n",
        "    print(feature, negatives)\n",
        "    tokenize_sentences_for_experimental_setting(feature, negatives, direction)"
      ],
      "metadata": {
        "id": "GUZZeqGrZB6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}