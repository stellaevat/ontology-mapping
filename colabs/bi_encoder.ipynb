{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxcS4WNloYk0Yqio+dmIfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellaevat/ontology-mapping/blob/main/colabs/bi_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zg1BOxf1KlAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b4bde5-b683-41a9-e8bd-d0b829e8bec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "Collecting evaluate\n",
            "  Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: faiss-gpu, dill, responses, multiprocess, accelerate, datasets, evaluate\n",
            "Successfully installed accelerate-0.27.2 datasets-2.17.1 dill-0.3.8 evaluate-0.4.1 faiss-gpu-1.7.2 multiprocess-0.70.16 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] datasets evaluate faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
      ],
      "metadata": {
        "id": "RQPDnoD3mcxf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import random\n",
        "import evaluate\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import get_scheduler, AutoModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, AdamW"
      ],
      "metadata": {
        "id": "YFVyasHU3Hk7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collate input dataset"
      ],
      "metadata": {
        "id": "P9cDB8uOQeGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_bi_tokenized_from_file(filepath):\n",
        "  s_input_ids, s_token_type_ids, s_attention_mask, t_input_ids, t_token_type_ids, t_attention_mask, labels = [], [], [], [], [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[\").split(\",\"))) for s in strings[:-1]]\n",
        "      s_input_id, s_token_type_id, s_attention, t_input_id, t_token_type_id, t_attention = lists\n",
        "      label = int(strings[-1])\n",
        "\n",
        "      s_input_ids.append(s_input_id)\n",
        "      s_token_type_ids.append(s_token_type_id)\n",
        "      s_attention_mask.append(s_attention)\n",
        "      t_input_ids.append(t_input_id)\n",
        "      t_token_type_ids.append(t_token_type_id)\n",
        "      t_attention_mask.append(t_attention)\n",
        "      labels.append(label)\n",
        "\n",
        "  source_tokenized = {\"input_ids\" : s_input_ids,\n",
        "                      \"token_type_ids\" : s_token_type_ids,\n",
        "                      \"attention_mask\" : s_attention_mask}\n",
        "  target_tokenized = {\"input_ids\" : t_input_ids,\n",
        "                      \"token_type_ids\" : t_token_type_ids,\n",
        "                      \"attention_mask\" : t_attention_mask}\n",
        "  return source_tokenized, target_tokenized, labels\n",
        "\n",
        "def read_cross_tokenized_from_file(filepath):\n",
        "  input_ids, token_type_ids, attention_mask, labels = [], [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[\").split(\",\"))) for s in strings[:-1]]\n",
        "      input_id, token_type_id, attention = lists\n",
        "      label = int(strings[-1])\n",
        "\n",
        "      input_ids.append(input_id)\n",
        "      token_type_ids.append(token_type_id)\n",
        "      attention_mask.append(attention)\n",
        "      labels.append(label)\n",
        "\n",
        "  tokenized = {\"input_ids\" : input_ids,\n",
        "               \"token_type_ids\" : token_type_ids,\n",
        "               \"attention_mask\" : attention_mask}\n",
        "  return tokenized, labels\n",
        "\n",
        "def onto_cross_tokenized_from_file(filepath):\n",
        "  input_ids, token_type_ids, attention_mask = [], [], []\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      strings = line.strip().split(\"],\")\n",
        "      lists = [list(map(int, s.strip(\"[\").split(\",\"))) for s in strings]\n",
        "      input_id, token_type_id, attention = lists\n",
        "\n",
        "      input_ids.append(input_id)\n",
        "      token_type_ids.append(token_type_id)\n",
        "      attention_mask.append(attention)\n",
        "\n",
        "  tokenized = {\"input_ids\" : input_ids,\n",
        "               \"token_type_ids\" : token_type_ids,\n",
        "               \"attention_mask\" : attention_mask}\n",
        "  return tokenized"
      ],
      "metadata": {
        "id": "hfr-jwzWfy5c"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "features = ['term', 'int', 'ext']\n",
        "direction = \"ncit2doid\"\n",
        "\n",
        "feature = features[0]\n",
        "negatives = negative_sampling[0]\n",
        "\n",
        "source_tokenized, target_tokenized, labels = read_bi_tokenized_from_file(f\"bi_tokenized_{feature}_{negatives}_{direction}.csv\")"
      ],
      "metadata": {
        "id": "9gJFN9tS5hg8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_source_target(Xi, source, target):\n",
        "  X_source = {k : [v[i] for i in Xi] for (k, v) in source.items()}\n",
        "  X_target = {k : [v[i] for i in Xi] for (k, v) in target.items()}\n",
        "  return X_source, X_target"
      ],
      "metadata": {
        "id": "H_CahO_vC5kY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_dataset(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "  dataset_train = Dataset.from_dict(X_train | {'labels' : y_train})\n",
        "  dataset_val = Dataset.from_dict(X_val | {'labels' : y_val})\n",
        "  dataset_test = Dataset.from_dict(X_test | {'labels' : y_test})\n",
        "  dataset = DatasetDict({'train' : dataset_train,\n",
        "                         'val' : dataset_val,\n",
        "                         'test' : dataset_test})\n",
        "  dataset.set_format(type=\"torch\")\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "00blfsjpIDl5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xi = np.arange(len(labels))\n",
        "y = np.array(labels)\n",
        "Xi_train_val, Xi_test, y_train_val, y_test = train_test_split(Xi, y, test_size=0.2, random_state=3)\n",
        "Xi_train, Xi_val, y_train, y_val = train_test_split(Xi_train_val, y_train_val, test_size=0.25, random_state=3)\n",
        "\n",
        "X_source_train, X_target_train = filter_source_target(Xi_train, source_tokenized, target_tokenized)\n",
        "X_source_val, X_target_val = filter_source_target(Xi_val, source_tokenized, target_tokenized)\n",
        "X_source_test, X_target_test = filter_source_target(Xi_test, source_tokenized, target_tokenized)\n",
        "\n",
        "print(f\"Train: {len(y_train)}\")\n",
        "print(f\"Validation: {len(y_val)}\")\n",
        "print(f\"Test: {len(y_test)}\")\n",
        "\n",
        "source_data = collate_dataset(X_source_train, X_source_val, X_source_test, y_train, y_val, y_test)\n",
        "target_data = collate_dataset(X_target_train, X_target_val, X_target_test, y_train, y_val, y_test)"
      ],
      "metadata": {
        "id": "ktcbbfRmFwuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f06e0a-7712-4cff-e039-2e36172d004c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 3860\n",
            "Validation: 1287\n",
            "Test: 1287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train end-to-end BERT-based bi-encoder"
      ],
      "metadata": {
        "id": "Mwy-uRdLOY69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiEncoderForSequenceClassification(torch.nn.Module):\n",
        "  def __init__(self, model_name, num_labels, id2label=None, label2id=None, token_embeddings_size=None, hidden_layer=-1):\n",
        "    super().__init__()\n",
        "    self.source_model = AutoModel.from_pretrained(model_name)\n",
        "    self.target_model = AutoModel.from_pretrained(model_name)\n",
        "    if token_embeddings_size:\n",
        "      self.source_model.resize_token_embeddings(token_embeddings_size)\n",
        "      self.target_model.resize_token_embeddings(token_embeddings_size)\n",
        "\n",
        "    self.num_labels = num_labels\n",
        "    self.hidden_layer = hidden_layer\n",
        "\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "    self.loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      s_input_ids=None, t_input_ids=None,\n",
        "      s_attention_mask=None, t_attention_mask=None,\n",
        "      s_token_type_ids=None, t_token_type_ids=None,\n",
        "      s_position_ids=None, t_position_ids=None,\n",
        "      s_head_mask=None, t_head_mask=None,\n",
        "      s_inputs_embeds=None, t_inputs_embeds=None,\n",
        "      labels=None\n",
        "    ):\n",
        "\n",
        "    source_outputs = self.source_model(\n",
        "      s_input_ids,\n",
        "      attention_mask=s_attention_mask,\n",
        "      token_type_ids=s_token_type_ids,\n",
        "      position_ids=s_position_ids,\n",
        "      head_mask=s_head_mask,\n",
        "      inputs_embeds=s_inputs_embeds,\n",
        "    )\n",
        "\n",
        "    target_outputs = self.target_model(\n",
        "      t_input_ids,\n",
        "      attention_mask=t_attention_mask,\n",
        "      token_type_ids=t_token_type_ids,\n",
        "      position_ids=t_position_ids,\n",
        "      head_mask=t_head_mask,\n",
        "      inputs_embeds=t_inputs_embeds,\n",
        "    )\n",
        "\n",
        "    pooled_source_outputs = self.dropout(source_outputs[1])\n",
        "    pooled_target_outputs = self.dropout(target_outputs[1])\n",
        "\n",
        "    # similarities = self.similarity(pooled_source_outputs, pooled_target_outputs)\n",
        "    # logits = self.sigmoid(similarities)\n",
        "    logits = torch.sum(pooled_source_outputs * pooled_target_outputs, dim=-1)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss = self.loss(logits.view(-1), labels.view(-1).float())\n",
        "\n",
        "    return SequenceClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "  def get_source_model_outputs(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    source_outputs = self.source_model(\n",
        "      input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "    return source_outputs[1]\n",
        "\n",
        "  def get_target_model_outputs(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    target_outputs = self.target_model(\n",
        "      input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "    return target_outputs[1]"
      ],
      "metadata": {
        "id": "xKdO0rN_wJsY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_determinism(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "1EeqaVxWfb_6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(epoch, loss, metrics):\n",
        "  print(f\"\\n\\nEPOCH {epoch}\\n\")\n",
        "  print(f\"Training loss: {loss}\")\n",
        "  pprint(metrics)\n",
        "  print()\n",
        "\n",
        "\n",
        "def evaluate_biencoder(model, source_data, target_data, batch_size=32):\n",
        "  eval_dataloader_index = DataLoader(Dataset.from_dict({'index' : range(len(source_data[\"val\"]))}), batch_size=batch_size)\n",
        "  metrics = [evaluate.load(\"accuracy\"), evaluate.load(\"precision\"), evaluate.load('recall'), evaluate.load('f1')]\n",
        "\n",
        "  model.eval()\n",
        "  for batch in eval_dataloader_index:\n",
        "    batch_index = list(batch[\"index\"])\n",
        "    source_batch = source_data[\"val\"][batch_index]\n",
        "    target_batch = target_data[\"val\"][batch_index]\n",
        "    labels = source_batch[\"labels\"]\n",
        "\n",
        "    source_batch = {\"s_\" + k: v.to(device) for (k, v) in source_batch.items() if k != \"labels\"}\n",
        "    target_batch = {\"t_\" + k: v.to(device) for (k, v) in target_batch.items() if k != \"labels\"}\n",
        "    params = source_batch | target_batch\n",
        "    params[\"labels\"] = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**params)\n",
        "\n",
        "    logits = outputs.logits.cpu()\n",
        "    loss = outputs.loss.item()\n",
        "\n",
        "    predictions = np.where(logits.squeeze() >= 0.5, 1, 0)\n",
        "    for metric in metrics:\n",
        "      metric.add_batch(predictions=predictions, references=labels.cpu())\n",
        "\n",
        "  metric_dict = {\"Validation loss\" : loss}\n",
        "  metric_dict.update(metrics[0].compute())\n",
        "  for metric in metrics[1:]:\n",
        "    metric_dict.update(metric.compute(average='macro'))\n",
        "\n",
        "  return metric_dict\n",
        "\n",
        "\n",
        "def train_biencoder(model, source_data, target_data, learning_rate=1e-5, epochs=3, batch_size=32):\n",
        "  train_dataloader_index = DataLoader(Dataset.from_dict({'index' : range(len(source_data[\"train\"]))}), shuffle=True, batch_size=batch_size)\n",
        "  num_training_steps = epochs * len(train_dataloader_index)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate) ### consider changing to adam (bertsubs)\n",
        "  scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps), position=0, leave=True)\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(1, epochs+1):\n",
        "    for i, batch in enumerate(train_dataloader_index):\n",
        "      batch_index = list(batch[\"index\"])\n",
        "      source_batch = source_data[\"train\"][batch_index]\n",
        "      target_batch = target_data[\"train\"][batch_index]\n",
        "      labels = source_batch[\"labels\"]\n",
        "\n",
        "      source_batch = {\"s_\" + k: v.to(device) for (k, v) in source_batch.items() if k != \"labels\"}\n",
        "      target_batch = {\"t_\" + k: v.to(device) for (k, v) in target_batch.items() if k != \"labels\"}\n",
        "      params = source_batch | target_batch\n",
        "      params[\"labels\"] = labels.to(device)\n",
        "\n",
        "      outputs = model(**params)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    metrics = evaluate_biencoder(model, source_data, target_data, batch_size=batch_size)\n",
        "    show_results(epoch, loss, metrics)"
      ],
      "metadata": {
        "id": "2QFIQmGhGda9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "full_determinism(seed=3)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "pretrained = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "model = BiEncoderForSequenceClassification(pretrained, num_labels=1)\n",
        "model.to(device)\n",
        "\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "yz4Gj-3cGv58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afd9198-f32f-49fc-b94d-17358cc99a12"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [10]\n",
        "batch_size = [2 ** i for i in range(4, 8)]\n",
        "learning_rate = [10 ** i for i in range(-6, -2)]\n",
        "param_grid = ParameterGrid({\"epochs\" : epochs, \"learning_rate\" : learning_rate, \"batch_size\" : batch_size})"
      ],
      "metadata": {
        "id": "Tp-9QTEhW_Cy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for params in param_grid:\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  train_biencoder(model, source_data, target_data, **params)"
      ],
      "metadata": {
        "id": "-IebuX4BGz0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe826cc-82a3-46c5-b17c-1432811302c6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 121/363 [01:21<02:24,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EPOCH 1\n",
            "\n",
            "Training loss: 3.600922107696533\n",
            "{'Validation loss': 0.5296826362609863,\n",
            " 'accuracy': 0.5322455322455323,\n",
            " 'f1': 0.4785475837932427,\n",
            " 'precision': 0.5574524129019433,\n",
            " 'recall': 0.5335002873799162}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 242/363 [02:56<01:13,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EPOCH 2\n",
            "\n",
            "Training loss: 0.7414628267288208\n",
            "{'Validation loss': 0.6765819191932678,\n",
            " 'accuracy': 0.7148407148407149,\n",
            " 'f1': 0.7147573653967516,\n",
            " 'precision': 0.7149792609351433,\n",
            " 'recall': 0.7147850929517057}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 363/363 [04:46<00:00,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EPOCH 3\n",
            "\n",
            "Training loss: 0.9027135968208313\n",
            "{'Validation loss': 0.32415246963500977,\n",
            " 'accuracy': 0.7117327117327117,\n",
            " 'f1': 0.711554388964234,\n",
            " 'precision': 0.7124475057035325,\n",
            " 'recall': 0.7118400525494704}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"state_dict_bi_encoder.pt\")"
      ],
      "metadata": {
        "id": "NQ9GPNppWXPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss"
      ],
      "metadata": {
        "id": "VEH4XQTPOV7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "neEMZu_DoL1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff116068-6fd3-4cda-816e-fd72d329efef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "662"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries_from = X_source_val\n",
        "database_from = X_target_val\n",
        "labels_from = y_val\n",
        "\n",
        "queries = queries_from[labels_from == 1]\n",
        "database = database_from[labels_from == 1]"
      ],
      "metadata": {
        "id": "QVlel1zmQXdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 0\n",
        "q_size = len(queries)\n",
        "db_size = len(database)"
      ],
      "metadata": {
        "id": "VLTbhwwrCCfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(list(queries[offset : offset + q_size]) + list(database[offset : offset + db_size]), padding=\"longest\")\n",
        "for (k, v) in tokenized.items():\n",
        "  tokenized[k] = torch.tensor(v, dtype=torch.int)\n",
        "\n",
        "tokenized_source = {k : v[:q_size].to(device) for (k, v) in tokenized.items()}\n",
        "tokenized_target = {k : v[q_size:].to(device) for (k, v) in tokenized.items()}"
      ],
      "metadata": {
        "id": "mYhBIqNLW9RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to cpu\n",
        "# tokenized_source, tokenized_target = None, None\n",
        "# gc.collect()\n",
        "# model.cpu()\n",
        "# tokenized_source = {k : v[:q_size].cpu() for (k, v) in tokenized.items()}\n",
        "# tokenized_target = {k : v[q_size:].cpu() for (k, v) in tokenized.items()}"
      ],
      "metadata": {
        "id": "rK3N34DZVXuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_queries = model.get_source_model_outputs(**tokenized_source)\n",
        "tokenized_database = model.get_source_model_outputs(**tokenized_target)"
      ],
      "metadata": {
        "id": "aRJNNxneTrgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "sim = torch.nn.CosineSimilarity(dim=-1)\n",
        "for i in range(q_size):\n",
        "  score = sim(tokenized_queries[i], tokenized_database[i])\n",
        "  if score >= 0.95:\n",
        "    count += 1\n",
        "    # print(score)\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "id": "4MMPNl586r22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = len(tokenized_database[0])\n",
        "xq = np.array(tokenized_queries.cpu().detach().numpy(), dtype=np.single)\n",
        "xb = np.array(tokenized_database.cpu().detach().numpy(), dtype=np.single)\n",
        "\n",
        "print(d, xq.shape, xb.shape)"
      ],
      "metadata": {
        "id": "PDsJZoPr-Kss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized = None\n",
        "# tokenized_source = None\n",
        "# tokenized_target = None\n",
        "tokenized_queries = None\n",
        "tokenized_database = None\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfl_rDXEb6mT",
        "outputId": "bd966031-1ddc-4bf9-fab9-a485eaedcc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = faiss.StandardGpuResources()  # use a single GPU\n",
        "\n",
        "index_flat = faiss.IndexFlatIP(d)   # build a flat (CPU) index\n",
        "\n",
        "index_flat.add(xb)                  # add vectors to the index\n",
        "# print(index_flat.ntotal)\n",
        "\n",
        "k = db_size                         # we want to see 100 nearest neighbors\n",
        "D, I = index_flat.search(xq, k)     # actual search\n",
        "# print(I[:5])                        # neighbors of the 5 first queries\n",
        "# print(D[:5])                        # similarities of the 5 first queries"
      ],
      "metadata": {
        "id": "JfhazGnF6tce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = []\n",
        "for i in range(q_size):\n",
        "  source, target = queries[offset + i], database[offset + i]\n",
        "  # print(\"\\nSource:\", source)\n",
        "  # print(\"Target:\", target)\n",
        "\n",
        "  for n, index in enumerate(I[i]):\n",
        "    # print(n + 1, database[offset + index])\n",
        "    if database[offset + index] == target:\n",
        "      ranks.append(n + 1)\n",
        "      # print(\"Rank found:\", n + 1)\n",
        "      break\n",
        "    # if n == k - 1:\n",
        "    #   print(\"Not found\")"
      ],
      "metadata": {
        "id": "RE5GxBXRgMbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(ranks, bins=max(ranks) // 10)"
      ],
      "metadata": {
        "id": "Z37GxccYwYS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_consider = 100\n",
        "in_first = ranks.count(1)\n",
        "in_ten = len([r for r in ranks if r <= 10])\n",
        "in_consider = len([r for r in ranks if r <= to_consider])\n",
        "in_all = len(ranks)\n",
        "mrr = np.mean(1 / np.array(ranks))\n",
        "\n",
        "print(f\"In first result: {in_first} ({(100 * in_first / len(ranks)):.1f}%)\")\n",
        "print(f\"In first 10 results: {in_ten} ({(100 * in_ten / len(ranks)):.1f}%)\")\n",
        "print(f\"In first {to_consider} results: {in_consider} ({(100 * in_consider / len(ranks)):.1f}%)\")\n",
        "print(f\"In first {k} results: {in_all} ({(100 * in_all / len(ranks)):.1f}%)\")\n",
        "print(f\"Lowest rank: {max(ranks)}\")\n",
        "print(f\"MRR: {mrr:.3f}\")\n",
        "\n",
        "## (can you load-unload model to file?)"
      ],
      "metadata": {
        "id": "IXFaM2QmeBTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}