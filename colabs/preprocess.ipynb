{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjHBuW/rfa1AYK9jCjHDo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellaevat/ontology-mapping/blob/main/colabs/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg1BOxf1KlAU"
      },
      "outputs": [],
      "source": [
        "!pip install pronto transformers datasets evaluate \\\n",
        "&& wget -O doid.obo https://gla-my.sharepoint.com/:u:/g/personal/2526934t_student_gla_ac_uk/EfUC_RdrfZdOsOrtmNATjuoBPDaIkSTUMyxJXyO2KKC6yw?download=1 \\\n",
        "&& wget -O ncit.obo https://gla-my.sharepoint.com/:u:/g/personal/2526934t_student_gla_ac_uk/ETmaJIC0fAlItdsp8WQxS_wBzKN_6x08EZrtsOxVnbzvSg?download=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pronto\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "YFVyasHU3Hk7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncit = pronto.Ontology(\"ncit.obo\")\n",
        "doid = pronto.Ontology(\"doid.obo\")"
      ],
      "metadata": {
        "id": "xbIac4iK3ZeK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get subsumptions from CSV file to a dictionary\n",
        "\n",
        "def get_subsumptions_from_file(filename):\n",
        "  subsumptions = {}\n",
        "  with open(filename) as f:\n",
        "    for line in f:\n",
        "      source_id, target_id = line.strip().split(',')\n",
        "      subsumptions[source_id] = target_id\n",
        "  return subsumptions\n",
        "\n",
        "subs_doid_to_ncit = get_subsumptions_from_file(\"doid_to_ncit.csv\")\n",
        "subs_ncit_to_doid = get_subsumptions_from_file(\"ncit_to_doid.csv\")"
      ],
      "metadata": {
        "id": "SwXk14NN4MtF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert relations to sentences\n",
        "\n",
        "> Currently considering parents, children & siblings for conceptual reasons, but could also take 'n-hop' appraoch, e.g. 1-hop only with parents and children, or 2-hop to include grandparents, grandchildren and siblings.\n",
        "\n",
        "> How do I incorporate the desired mapping for training? Should I incorporate both all this AND target info, or too much? Could be SELF + desired relatives instead, or SELF + PARENT + DESIRED PARENT, etc."
      ],
      "metadata": {
        "id": "4kBAYe8J3pRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence from the given entity, containing its direct parents, children & siblings\n",
        "\n",
        "def get_term_plus_relatives(entity_id, onto):\n",
        "  markers = [\"[SELF]\", \"[/SELF]\", \"[SUP]\", \"[/SUP]\", \"[SUB]\", \"[/SUB]\", \"[SIBL]\", \"[/SIBL]\"]\n",
        "  term_in, term_out, sup_in, sup_out, sub_in, sub_out, sibl_in, sibl_out = markers\n",
        "\n",
        "  term = onto.get_term(entity_id)\n",
        "  parents = term.superclasses(distance=1, with_self=False)\n",
        "  children = term.subclasses(distance=1, with_self=False)\n",
        "  siblings = set()\n",
        "  for parent in parents:\n",
        "    siblings.update(set(parent.subclasses(distance=1, with_self=False)))\n",
        "  siblings.remove(term)\n",
        "\n",
        "  term_plus_relatives = [term_in, term.name, term_out]\n",
        "  for parent in parents:\n",
        "    term_plus_relatives.extend([sup_in, parent.name, sup_out])\n",
        "  for child in children:\n",
        "    term_plus_relatives.extend([sub_in, child.name, sub_out])\n",
        "  for sibling in siblings:\n",
        "    term_plus_relatives.extend([sibl_in, sibling.name, sibl_out])\n",
        "\n",
        "  return \" \".join(term_plus_relatives)"
      ],
      "metadata": {
        "id": "Yvx837pI3q2Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_term_plus_relatives(\"DOID:0050159\", doid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa5kp9Pg3t8C",
        "outputId": "c3cd4373-4466-4baf-efd6-ed2c08d8aaee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SELF] lymphoid interstitial pneumonia [/SELF] [SUP] idiopathic interstitial pneumonia [/SUP] [SIBL] cryptogenic organizing pneumonia [/SIBL] [SIBL] desquamative interstitial pneumonia [/SIBL] [SIBL] nonspecific interstitial pneumonia [/SIBL] [SIBL] acute interstitial pneumonia [/SIBL]\n"
          ]
        }
      ]
    }
  ]
}