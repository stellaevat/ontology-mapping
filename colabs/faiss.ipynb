{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellaevat/ontology-mapping/blob/main/colabs/faiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg1BOxf1KlAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4150eeef-3132-459f-fae8-aaf11e831895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fyACOW5XgpgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1cae89-9efe-40f3-c752-664da0b82f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
      ],
      "metadata": {
        "id": "RQPDnoD3mcxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import itertools\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YFVyasHU3Hk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read/Write to file"
      ],
      "metadata": {
        "id": "6uM7hsKBdLSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_bi_embeddings_from_file(filepath):\n",
        "  indices, sources, targets = [], [], []\n",
        "\n",
        "  with open(filepath) as f:\n",
        "    for line in tqdm(f):\n",
        "      strings = line.strip().split(\",[\")\n",
        "      lists = [list(map(float, s.strip(\"[]\").split(\",\"))) for s in strings[1:]]\n",
        "      source, target = lists\n",
        "      index = int(strings[0])\n",
        "\n",
        "      indices.append(index)\n",
        "      sources.append(source)\n",
        "      targets.append(target)\n",
        "\n",
        "  bi_embeddings = {\"indices\" : indices,\n",
        "                   \"sources\" : np.array(sources, dtype=np.single),\n",
        "                   \"targets\" : np.array(targets, dtype=np.single)}\n",
        "  return bi_embeddings\n",
        "\n",
        "def read_onto_embeddings_from_file(filepath):\n",
        "  indices, embeddings = [], None\n",
        "\n",
        "  with open(filepath) as f:\n",
        "    for line in tqdm(f):\n",
        "      strings = line.strip().split(\",[\")\n",
        "      embedding = np.array(list(map(float, strings[1].strip(\"[]\").split(\",\"))), dtype=np.single).reshape(1,-1)\n",
        "      index = int(strings[0])\n",
        "\n",
        "      indices.append(index)\n",
        "      embeddings = embedding if embeddings is None else np.concatenate((embeddings, embedding), axis=0, dtype=np.single)\n",
        "\n",
        "  onto_embeddings = {\"indices\"    : indices,\n",
        "                     \"embeddings\" : embeddings}\n",
        "  return onto_embeddings"
      ],
      "metadata": {
        "id": "cnmwfmWTdOqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss"
      ],
      "metadata": {
        "id": "VEH4XQTPOV7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_real_ranks(I, bi_embeddings, db_embeddings):\n",
        "  ranks = []\n",
        "  for i, target_embed in tqdm(enumerate(bi_embeddings[\"targets\"])):\n",
        "    for rank, j in enumerate(I[i]):\n",
        "      ranked_embed = db_embeddings[j]\n",
        "      if np.array_equal(target_embed, ranked_embed):\n",
        "        ranks.append(rank + 1)\n",
        "        break\n",
        "  return ranks\n",
        "\n",
        "def plot_ranks(ranks, feature, negatives, database):\n",
        "  plt.figure()\n",
        "  plt.hist(ranks, bins=max(ranks) // 100)\n",
        "  plt.xlabel(\"Subsumer rank\")\n",
        "  plt.ylabel(\"No. of queries\")\n",
        "  plt.title(f\"Correct subsumer rank distribution across queries ({feature}, {negatives}, {database})\")\n",
        "  plt.savefig(f\"bi_ranking_histogram_{feature}_{negatives}_{database}.png\")\n",
        "\n",
        "def compute_result(ranks):\n",
        "  result = {}\n",
        "\n",
        "  raw_at_rank = lambda x: len([r for r in ranks if r <= x])\n",
        "  hits_at_rank = lambda x: x / len(ranks)\n",
        "  for r in [1, 5, 10, 100, 1000]:\n",
        "    score = hits_at_rank(raw_at_rank(r))\n",
        "    result[f\"H@{r}\"] = score\n",
        "\n",
        "  result[\"MRR\"]     = np.mean(1 / np.array(ranks))\n",
        "  result[\"Highest\"] = np.min(ranks)\n",
        "  result[\"Median\"]  = np.median(ranks)\n",
        "  result[\"Mean\"]    = np.mean(ranks)\n",
        "  result[\"Lowest\"]  = np.max(ranks)\n",
        "\n",
        "  return result\n",
        "\n",
        "def pretty_print_result(result, feature, negatives, database):\n",
        "  result_str = f\"\\nPerformance on ({feature}, {negatives}, {database}):\\n\\n\"\n",
        "\n",
        "  for (name, value) in result.items():\n",
        "    if name.startswith(\"H@\"):\n",
        "      result_str += f\"{name:<6} = {value:.3f}\\n\"\n",
        "    elif name == \"MRR\":\n",
        "      result_str += f\"MRR    = {value:.3f}\\n\\n\"\n",
        "    elif not isinstance(value, str):\n",
        "      result_str += f\"{name}:\" + \" \" * (13 - len(name)) + f\"{value:.1f}\\n\"\n",
        "\n",
        "  result_str += \"\\n\"\n",
        "  print(result_str)"
      ],
      "metadata": {
        "id": "gse87owrvf_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"\"\n",
        "dir = \"/content/drive/MyDrive/embeddings/\"\n",
        "features = ['term', 'int', 'ext']\n",
        "negative_sampling = ['random', 'multi', 'neighbour']\n",
        "columns = [\"database\", \"feature\", \"negatives\", \"H@1\", \"H@5\", \"H@10\", \"H@50\", \"H@100\", \"H@1000\", \"MRR\", \"Highest\", \"Median\", \"Mean\", \"Lowest\"]\n",
        "results_df = pd.DataFrame(columns=columns)\n",
        "all_ranks = {}\n",
        "\n",
        "for (feature, negatives) in list(itertools.product(features, negative_sampling))[3:]:\n",
        "  bi_embeddings = read_bi_embeddings_from_file(f\"{dir}bi_test_embeddings_{feature}_{negatives}.csv\")\n",
        "  random_embeddings = read_onto_embeddings_from_file(f\"{dir}random_embeddings_{feature}_{negatives}.csv\")\n",
        "  hard_embeddings = read_onto_embeddings_from_file(f\"{dir}hard_embeddings_{feature}_{negatives}.csv\")\n",
        "  onto_embeddings = read_onto_embeddings_from_file(f\"{dir}doid_embeddings_{feature}_{negatives}.csv\")\n",
        "  databases = {\"hard\" : hard_embeddings, \"onto\" : onto_embeddings, \"random\" : random_embeddings}\n",
        "\n",
        "  for (db_type, db_embeddings) in databases.items():\n",
        "    if db_embeddings is None:\n",
        "      continue\n",
        "\n",
        "    print()\n",
        "    print(feature, negatives, db_type)\n",
        "\n",
        "    xq = bi_embeddings[\"sources\"]\n",
        "    xb = db_embeddings[\"embeddings\"]\n",
        "\n",
        "    xb = np.concatenate((xb, bi_embeddings[\"targets\"]), axis=0)\n",
        "    print(f\"Database ({db_type}) contains {len(xb)} terms\")\n",
        "\n",
        "    d = xb[0].shape[0]\n",
        "    q_size = len(xq)\n",
        "    db_size = len(xb)\n",
        "\n",
        "    res = faiss.StandardGpuResources()\n",
        "    index_flat = faiss.IndexFlatIP(d)\n",
        "    index_flat.add(xb)\n",
        "    D, I = index_flat.search(xq, db_size)\n",
        "\n",
        "    ranks = get_real_ranks(I, bi_embeddings, xb)\n",
        "    if not ranks:\n",
        "      print(\"No ranks!\\n\")\n",
        "      continue\n",
        "    else:\n",
        "      print(len(ranks))\n",
        "\n",
        "    plot_ranks(ranks, feature, negatives, db_type)\n",
        "    result = {\"database\" : db_type, \"feature\" : feature, \"negatives\" : negatives}\n",
        "    result = result | compute_result(ranks)\n",
        "    # pretty_print_result(result, feature, negatives, db_type)\n",
        "\n",
        "    results_df.loc[len(results_df)] = result\n",
        "    results_df.sort_values(by=[\"database\", \"feature\", \"negatives\"]).to_csv(\"bi_ranking_metrics.csv\", index=False)\n",
        "\n",
        "    all_ranks[(database, feature, negatives)] = ranks\n",
        "    with open(\"bi_ranks.csv\", \"a\") as f:\n",
        "      f.write(f\"{database},{feature},{negatives},{\",\".join(ranks)}\\n\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "FI95Q0Q_dTEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}